# 3. Movement Ecology Data Analysis in R

<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-refresh-toc -->
**Table of Contents**

- [3. Movement Ecology Data Analysis in R](#3-movement-ecology-data-analysis-in-r)
    - [3.1  Introduction to R](#31--introduction-to-r)
        - [3.1.1 Features of R](#311-features-of-r)
    - [3.2 An Introduction to Movement Ecology in R](#32-an-introduction-to-movement-ecology-in-r)
        - [3.2.1 Topic 1: Trajectories in R](#321-topic-1-trajectories-in-r)
            - [Exercise 1](#exercise-1)
        - [3.2.2 Topic 2: Cleaning trajectories](#322-topic-2-cleaning-trajectories)
            - [Exercise 2](#exercise-2)
        - [3.2.3 Topic 3: Interpolation in time and space](#323-topic-3-interpolation-in-time-and-space)
            - [Exercise 3](#exercise-3)
        - [3.2.4 Topic 4: Home ranges](#324-topic-4-home-ranges)
            - [Exercise 4](#exercise-4)
        - [3.2.5 Topic 5: Random walks](#325-topic-5-random-walks)
            - [Exercise 5](#exercise-5)
        - [3.2.6 Topic 6: Habitat selection (DEMO)](#326-topic-6-habitat-selection-demo)
    - [3.3 There and back again: Connecting PostGIS and R](#33-there-and-back-again-connecting-postgis-and-r)
        - [3.3.1 Topic 7: Import data from PostGIS with `RPostgreSQL`](#331-topic-7-import-data-from-postgis-with-rpostgresql)
            - [Exercise 7: Import steps as ltraj](#exercise-7-import-steps-as-ltraj)
        - [3.3.2 Topic 8: Export data to PostGIS with `RPostgreSQL`](#332-topic-8-export-data-to-postgis-with-rpostgresql)
            - [Exercise 8a: Export ltraj to the data base as steps](#exercise-8a-export-ltraj-to-the-data-base-as-steps)
            - [Exercise 8b: What about shapefiles?](#exercise-8b-what-about-shapefiles)
        - [3.3.3 Topic 9: Introducing `rpostgis`](#333-topic-9-introducing-rpostgis)
            - [Exercise 9a: Import roads](#exercise-9a-import-roads)
            - [Exercise 9b: Export to PostGIS](#exercise-9b-export-to-postgis)
        - [3.3.4 Topic 10: More of `rpostgis`](#334-topic-10-more-of-rpostgis)
        - [3.3.5 Topic 11: Let's not forget about trajectories, here comes `rpostgisLT`](#335-topic-11-lets-not-forget-about-trajectories-here-comes-rpostgislt)
            - [Exercice 11: Create a trajectory in the DB with `asPgtraj`](#exercice-11-create-a-trajectory-in-the-db-with-aspgtraj)
        - [3.3.6 Closure…](#336-closure)
    - [3.4 Extending PostGIS with Pl/R (DEMO)](#34-extending-postgis-with-plr-demo)
        - [3.4.1 Getting Started with Pl/R](#341-getting-started-with-plr)
        - [3.4.2 In the Middle of the Night](#342-in-the-middle-of-the-night)
        - [3.4.2 Extending the Home Range Concept](#342-extending-the-home-range-concept)
        - [3.4.4 Concluding remarks about Pl/R](#344-concluding-remarks-about-plr)
    - [3.5 Recap exercises](#35-recap-exercises)

<!-- markdown-toc end -->


## 3.1  Introduction to R

Until now, we explored the wide set of tools that PostgreSQL and
PostGIS offer to process and analyse tracking data. Nevertheless, a
database is not specifically designed to perform advanced statistical
analysis or to implement complex analytical algorithms, which are key
elements to extract scientific knowledge from the data for both
fundamental and applied research. In fact, these functionalities must
be part of an information system that aims at a proper handling of
wildlife tracking data. The possibility of a tighter integration of
analytical functions with the database is particularly interesting
because the availability of large amounts of information from the new
generation sensors blurs the boundary between data analysis and data
management. Tasks like outlier filtering, real-time detection of
specific events (e.g. virtual fencing), or meta-analysis (analysis of
results of a first analytical step, e.g. variation in home range size
in the different months of a year) are clearly in the overlapping area
between data analysis and management.

To analyse data, and movement data in particular, R is probably the
best solution. R is an open source programming language and
environment for statistical computing and graphics.  R is a free and
open-source software. You can thus *freely* use it, *freely*
distribute it, and *freely* modify and redistribute it.  It is a
popular choice for data analysis in academics, with its popularity for
ecological research increasing rapidly.  R is available on the [R
Project website](https://www.r-project.org).


### 3.1.1 Features of R

* R comes with an entire ecosystem of packages, which provide
  additional features to the software. Prominent packages are hosted
  on CRAN, which has now more than 12,000 packages. Some packages
  provide generic functionalities that are useful in most cases
  (modeling, graphs, etc.), other are heavily specialized on specific
  tasks (e.g. parallel processing, connection to a database, habitat
  selection, finances, etc.). The ecosystem is now so rich that if a
  statistical tool or feature has been developed, the chances that it
  exists as a R package are fairly high. You can check [CRAN task
  views](https://cran.r-project.org/web/views/) to find out what
  packages are available on specific subjects.

* R can handle pretty much any type of input, from plain text files
  (`.txt` or `.csv` files for instance), Excel spreadsheets, database
  tables, or even data copied in the clipboard. R can also scrape
  information from the web, either by direct download of files, or by
  parsing web pages.

* R allows modeling at an advanced level. Pretty much all statistical
  models are available directly in base R or in external packages,
  such as Generalized linear models (GLM), mixed models (with random
  factors), Generalized additive models (GAM), Random forest, machine
  learning, etc.

* R has truly amazing capabilities when it comes to graphics, with
  incredible flexibility, and amazing results. Interested users may
  want to have a look at the `ggplot2` package (see next point).

* The [`tidyverse`](https://tidyverse.org/) is a cohesive set of
  packages on its own. It notably provides a graphic system that
  greatly improve base R capabilities by implementing a grammar of
  graphics (package `ggplot2`), and also provides a very clean
  approach of dealing with tabular data that relies on database
  principles (package `dplyr`). Although familiarity with these
  packages brings many benefits, this tutorial does not cover the
  `tidyverse`.

* As a statistical engine, R unleashes advanced uses of its
  capabilities through external applications or media. Most notably, R
  allows for reproducible science through the use of
  [RMarkdown](http://rmarkdown.rstudio.com/) files that automatically
  generate reports, as well as the development of web applications
  that allows dynamic interface to data and statistical models through
  the [Shiny package](https://cran.r-project.org/package=shiny).

An introductory tutorial can be found
[here](https://ase-research.org/R/intro/), and present basic
capabilities of R, and how to get started with different data types
and graphics.


## 3.2 An Introduction to Movement Ecology in R

This section heavily relies on the database created so far. It is thus
expected that a full tracking database is ready and functional. Check
the previous section ([*Backup of the database created in SECTION
2*](../section_2/l2_dbmovement.md#backup-of-the-database-created-in-section-2))
for details on how to restore a full database, with a link to a
complete backup.

For the analysis of animal tracking data, we often use functions from
the adehabitat family which consists of four packages:

* `adehabitatMA`: management of raster maps;
* `adehabitatLT`: analysis of trajectories;
* `adehabitatHR`: home range estimation;
* `adehabitatHS`: habitat-selection analysis.

Other packages will be useful for this lesson:

* `sp`: spatial data in R;
* `raster`: special package for raster data;
* `rgdal`: bindings to GDAL;
* `lubridate`: management of dates/timestamps in R;
* `rpostgis`: interface between R and PostGIS;
* `rpostgisLT`: transfer animal trajectories between R and PostGIS.

We will also need the package `devtools` to access packages hosted on
GitHub.

Let's install all necessary packages right now:

```r
install.packages("adehabitatHS")
install.packages("raster")
install.packages("lubridate")
install.packages("devtools")
```

Finally, we install the packages `basr`, `hab` and `rpostgis` directly
from GitHub, with the help of `devtools`; we also need to install the
development version of `rpostgisLT`:

```r
library("devtools")
install_github("basille/basr")
install_github("basille/hab")
install_github("mablab/rpostgis")
install_github("mablab/rpostgisLT", ref = "dev")
```


### 3.2.1 Topic 1: Trajectories in R

For the analysis of movement data, we will mostly rely on the class
`ltraj` from the `adehabitatLT` package. This class is intended to
store trajectories of animals. Trajectories of type I correspond to
trajectories for which the time has not been recorded (e.g.  sampling
of tracks in the snow). Trajectories of type II correspond to
trajectories for which the time is available for each relocation
(mainly GPS and radio-tracking), that is the focus of this week.

Let's build step by step a `ltraj` object using an example data set
provided in the package (wild boar tracking data in southern
France). We first load the package and the data:

```r
library("hab")
data(puechabonsp)
```

Then check what we have in this dataset:

```r
names(puechabonsp)
```

We first extract the maps themselves, and plot them:

```r
(map <- puechabonsp$map)
mimage(puechabonsp$map)
```

We now extract the location dataset, and explore it a bit:

```r
locs <- puechabonsp$relocs
summary(locs)
head(locs)
image(puechabonsp$map)
points(locs, pch = 20, col = locs$Name)
```

The column for dates is not stored in a class that is recognized as
such in R. We need to do it now, using the package `lubridate`:


```r
class(locs$Date)
library("lubridate")
locs$Date <- ymd(paste0("19", locs$Date), tz = "UTC")
head(locs$Date)
class(locs$Date)
```

We can now store the data into a `ltraj` object, using the `as.ltraj`
function, and explore the structure of this object:

```r
(tr1 <- as.ltraj(coordinates(locs), date = locs$Date, id = locs$Name))
class(tr1)
tr1[[1]]
str(tr1[[1]])
```

This allows us to display each individual trajectory:

```r
plot(tr1, spixdf = map)
plot(tr1, by = "none", spixdf = map, addpoints = FALSE, final = FALSE, lpar = list(col = c(Brock = "blue", Calou = "orange", Chou = "green", Jean = "red"), lwd = 3))
```

We can quickly look at the distribution of step lengths:

```r
dtr1 <- ld(tr1)
head(dtr1)
hist(dtr1$dist, breaks = 20, freq = FALSE, xlab = "Step length", main = "Histogram of wild boar step lengths")
lines(density(dtr1$dist, na.rm = TRUE), lwd = 3)
```

... and turning angles (using a circular histogram):

```r
rose.diag(na.omit(dtr1$rel.angle), bins = 18, prop = 1.5)
```


#### Exercise 1

You will now import in R all location data from the database, and
build them into a `ltraj` object. Import and export will be the
subject of another lesson, let's just do it now once and for all
using `rpostgis` and `rgdal` without further explanation:

```r
library("rpostgis")
library("rgdal")
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "gps_tracking_db", host = "localhost", user = "<user>", password = "<password>")
locs2 <- spTransform(pgGetGeom(con, c("main", "gps_data_animals"), clauses = "WHERE gps_validity_code = 1"), CRS("+init=epsg:32632"))
tz(locs2$acquisition_time) <- "UTC"
head(locs2)
```

You will now build the `ltraj`, with the name `tr2`, and only keep
individual #5. If you manage to do it properly, try to explore
dynamically the trajectory with the function `trajdyn`, and play with
the different parameters (run `X11` first if you're using RStudio):

```r
X11()                                   # Optional for RStudio
trajdyn(tr2)
```


### 3.2.2 Topic 2: Cleaning trajectories

Assuming that there are only valid points in the trajectory, there is
generally two things that need to be addressed: missing values and
exact timestamps. Both are related to the temporal aspect of
movement. Let's thus look at the temporal interval between every
location, in days:

```r
plotltr(tr1, "dt/3600/24")
```

For Chou, there is a huge gap in the middle, because the individual
was monitored two successive summers. We thus create two bursts for
this individual, one per summer:

```r
tr1 <- cutltraj(tr1, "dt > 100*3600*24", nextr = TRUE)
plotltr(tr1, "dt/3600/24")
```

As we can see, although there is supposed to be one day in between
successive locations, there can be up to 8 days in between. To
'regularize' the trajectory, we thus need to add missing values (NAs)
in the trajectory for those days where there is no data. For this, we
need to use the function `setNA` with a reference date (which will be
the oldest date of the dataset):

```r
min(locs$Date)
(ref <- dmy("29071992", tz = "UTC"))
(tr1 <- setNA(tr1, ref, 1, units = "day"))
plotltr(tr1, "dt/3600/24")
head(tr1[[1]])
```

The intervals are now perfectly regular, with one day in between
successive relocations. We can also see where NAs have been placed
(this could be the subject of a separate analysis!):

```r
plotNAltraj(tr1, addlines = FALSE, ppar = list(pch = 15))
```


#### Exercise 2

You will first start by checking and adding missing data in the roe
deer dataset (`tr2`). Are the missing data randomly distributed in the
trajectory? (look up the function `runsNAltraj`)

Now, the wild boar dataset does not include the time of the day: the
temporal precision is the day. In the case of more precise times, like
for the roe deer case, the recorded time will generally not be the
exact scheduled time: the GPS needs some time to find a location. For
instance, if the GPS is programmed to take one relocation precisely at
2:00, it will probably do it a few minutes later, maybe at 2:03:24. To
have a perfectly regular trajectory, we thus need to round the
recorded times to the expected times. You will do it on the roe deer
dataset (`tr2`), using the `sett0` function.


### 3.2.3 Topic 3: Interpolation in time and space 

In this section, we are going to interpolate in time and in
space. Let's start by interpolation in time, i.e. linear
interpoloation of missing locations:

```r
(tr1t <- redisltraj(na.omit(tr1), 3600*24, type = "time"))
col <- ifelse(is.na(tr1[[2]]$x), "white", "black")
plot(tr1t[2], ppar = list(pch = 21, col = "black", bg = list(Calou.1 = col[-length(col)])))
```

In a second step, we interpolate in space, that is we rediscretize the
trajectory with constant step length of 100 m:

```r
summary(dtr1$dist)
(tr1s <- redisltraj(tr1, 100, nnew = 10))
plot(tr1s[2])
```


#### Exercise 3

You will now rediscretize roe deer trajectories with constant step
length approximately equal to the median step length. Does the result
make sense?


### 3.2.4 Topic 4: Home ranges

We will now look at home ranges. We will first start with classical
home ranges methods that do not take movement into account, before
incorporating movement information along steps. Let us start with
Minimum Convex Polygons:

```r
tr1sp <- ltraj2spdf(tr1)
summary(tr1sp)
mcp1 <- mcp(tr1sp["id"])
plot(mcp1)
plot(tr1sp, col = as.data.frame(tr1sp)[, "id"], add = TRUE)
```

A second classical approach is kernel home ranges, which estimates a
utilization distribution:

```r
kud1 <- kernelUD(tr1sp["id"], grid = 100, same4all = TRUE)
image(kud1)
image(kud1[[2]])
plot(mcp1[2, ], add = TRUE)
```

Finally, we can estimate home ranges using the Brownian bridge kernel
approach, which incorporates information along movement steps:

```r
liker(tr1, sig2 = 50, rangesig1 = c(1, 10))
kbb1 <- kernelbb(tr1, sig1 = 2, sig2 = 50, grid = 100, same4all = TRUE)
image(kbb1)
image(kbb1[[2]])
plot(mcp1[2, ], add = TRUE)
```

#### Exercise 4

Simply estimate Brownian bridge kernels on the roe deer data set, and
compare it to MCP!



### 3.2.5 Topic 5: Random walks

On Wednesday morning, we learned the importance of random walk theory
in movement ecology. Now is the time to put it in practice! We will
start by simulating a simple random walk for 1000 steps

```r
rw1 <- simm.crw(1:1000, r = 0, burst = "RW r = 0")
plot(rw1, addpoints = FALSE)
```

Let's have a look at step length and turning angle distributions:

```r
drw1 <- ld(rw1)
hist(drw1$dist, breaks = 20, freq = FALSE, xlab = "Step length", main = "Histogram of RW step lengths")
lines(density(drw1$dist, na.rm = TRUE), lwd = 3)
rose.diag(na.omit(drw1$rel.angle), bins = 18, prop = 3)
```

We can see the diffusive property of the random walk by increasing the
number of steps to 100000:

```r
rw2 <- simm.crw(1:100000, r = 0, burst = "RW 100000 steps")
plot(rw2, addpoints = FALSE)
```

The next step is to simulate correlated random walks by increasing the
concentration parameter of turning angles (`r`). Remember that the
simple RW is a specific case of CRW:

```r
crw0 <- simm.crw(1:1000, r = 0, id = "CRW0 r = 0 (RW)", h = 8)
crw1 <- simm.crw(1:1000, r = 0.6, id = "CRW1 r = 0.6", h = 5)
crw2 <- simm.crw(1:1000, r = 0.9, id = "CRW2 r = 0.9", h = 2)
crw3 <- simm.crw(1:1000, r = 0.99, id = "CRW3 r = 0.99")
mov <- c(crw0, crw1, crw2, crw3)
plot(mov, addpoints = FALSE)
```

We can also check step length and turning angle distributions:

```r
dcrw2 <- ld(crw2)
hist(dcrw2$dist, breaks = 20, freq = FALSE, xlab = "Step length", main = "Histogram of CRW step lengths")
lines(density(dcrw2$dist, na.rm = TRUE), lwd = 3)
rose.diag(na.omit(dcrw2$rel.angle), bins = 18, prop = 1.5)
```

#### Exercise 5

Now you will generate a Brownian bridge from the point (0,0) to the
point (100,100) using the function `simm.bb`. Try to vary the number
of steps, as well as the end point.

In a second step, simulate several Levy walks using the `simm.levy`
and vary the different parameters to understand their effect.



### 3.2.6 Topic 6: Habitat selection (DEMO)

In this section, we will see how to perform a simple approach of Step
Selection Functions. The first step is to create random steps, by
drawing random step lengths and random turning angles within the set
of observed steps:

```r
rdtr2 <- rdSteps(tr2, reproducible = TRUE)
head(rdtr2)
```

The result is a data frame, that we convert to a
`SpatialPointsDataFrame` using the coordinates at the end of the step:

```r
coordinates(rdtr2) <- data.frame(x = rdtr2$x + rdtr2$dx, y = rdtr2$y + rdtr2$dy)
proj4string(rdtr2) <- "+init=epsg:32632"
plot(rdtr2, pch = 20, cex = 0.2)
segments(x0 = rdtr2@data$x, y0 = rdtr2@data$y, x1 = rdtr2@data$x + rdtr2$dx, y = rdtr2@data$y + rdtr2$dy)
rd1tr2 <- subset(rdtr2, case == 1)
segments(x0 = rd1tr2@data$x, y0 = rd1tr2@data$y, x1 = rd1tr2@data$x + rd1tr2$dx, y = rd1tr2@data$y + rd1tr2$dy, col = "red")
```

The next step is to intersect all steps (observed and random) to the
environmental variables of choice. Here we work with the land cover
(Corine Land Cover) and the elevation (Digital Elevation Model), that
we import with `rpostgis` without further explanation:

```r
corine <- pgGetRast(con, c("env_data", "corine_land_cover"))
plot(corine)

dem <- pgGetRast(con, c("env_data", "srtm_dem"))
plot(dem)
```

We now extract the environmental variables at the end of the step; we
also reclassify the land cover type into a limited number of
categories:

```r
library("raster")
rdtr2@data <- data.frame(rdtr2@data, dem = extract(dem, rdtr2), corine = extract(corine, rdtr2))
table(rdtr2$corine)

library("basr")
(matsimp <- matrix(c(2, 18, 20, 21, 23, 24, 25, 26, 27, 29, 31, 32, "open", "agri", "agri", "agri", "forest", "forest", "forest", "open", "open", "open", "open", "open"), ncol = 2))
rdtr2$corine <- reclass(rdtr2$corine, matsimp, factor = TRUE)
table(rdtr2$corine)
```

Finally, given that the land cover type is a factor (i.e. qualitative
variable), we need to convert it to a set of dummy variables, one of
which will be dropped and used as a reference later:

```r
library("ade4")
rdtr2@data <- data.frame(rdtr2@data, acm.disjonctif(rdtr2@data[, "corine", drop = FALSE]))
head(rdtr2)
```

We can now run the conditional logistic regression on each strata (one
observed step + 10 associated random steps) to check the selection on
these two variables:

```r
library("survival")
ssf1 <- clogit(case ~ dem + corine.forest + corine.open + strata(strata), data = rdtr2, method = "breslow")
summary(ssf1)
```


## 3.3 There and back again: Connecting PostGIS and R

In the previous section, we directly imported data from the PostGIS
database, without paying real attention to the commands. Now in this
section, we are going to investigate how to import and export data
from PostGIS to R and back.

For this lesson, we are going to use the following packages:

* `RPostgreSQL`: standard connection from R to PostgreSQL;
* `rpostgis`: interface between R and PostGIS;
* `rpostgisLT`: transfer animal trajectories between R and PostGIS;
* `sf`: support for simple features (spatial vector data standard) in
  R.


### 3.3.1 Topic 7: Import data from PostGIS with `RPostgreSQL`

The most direct approach is to use `RPostgreSQL` to connect to the
database for the whole session, evaluate SQL queries, and import and
export data frames.  The first step involves opening the connection,
and may require username (`user`) and password (`password`) as
additional parameters:

```r
library("RPostgreSQL")
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "gps_tracking_db", host = "localhost", user = "<user>", password = "<password>")
dbListTables(con)
```

The package provide a generic function to import a given table into a
`data.frame`:

```r
(tab <- dbReadTable(con, c("main", "animals")))
class(tab)
str(tab)
```

However, this function is not able to handle geometry columns
properly:

```r
tab <- dbReadTable(con, c("main", "gps_data_animals"))
head(tab)
str(tab)
```

In this case, we need to write a specific query to get only the fields
we want, and in the format we want:

```r
(query <- "SELECT * FROM main.animals;")
class(query)
(tab <- dbGetQuery(con, query))
class(tab)
str(tab)
```

Let's do it now on the GPS data, and extract the projected coordinates
(UTM):

```r
dbListFields(con, c("main", "gps_data_animals"))
(query <- "SELECT animals_id, acquisition_time, longitude, latitude, ST_X(ST_Transform(geom, 32632)) AS x, ST_Y(ST_Transform(geom, 32632)) AS y, roads_dist, altitude_srtm
FROM main.gps_data_animals WHERE gps_validity_code = 1")
locs <- dbGetQuery(con, query)
head(locs)
class(locs)
dim(locs)
```

We can now convert this data frame into a proper spatial object in R:

```r
library("sp")
coordinates(locs) <- c("x", "y")
class(locs)
summary(locs)
```

We could also extract the SRID directly from PostGIS to declare it in
our spatial object, but in this case, we projected the data on the fly
to the UTM coordinates:

```r
query <- "SELECT DISTINCT(ST_SRID(geom)) FROM main.gps_data_animals WHERE geom IS NOT NULL;"
dbGetQuery(con, query)
proj4string(locs) <- CRS("+init=epsg:32632")
summary(locs)
plot(locs, xlim = c(653000, 663000), ylim = c(5093000, 5103000))
```


#### Exercise 7: Import steps as ltraj

You should now be able to import locations from the database, and
convert them to a proper `ltraj` object.


### 3.3.2 Topic 8: Export data to PostGIS with `RPostgreSQL`

`RPostgreSQL` also provides functions to export data back to PostGIS.
Let's first load the `puechcirc` dataset as an example:

```r
library("hab")
data(puechcirc)
puechcirc
dfpu <- ld(puechcirc)
head(dfpu)
dl(dfpu)
```

We first create a test schema in the database, using the function
`dbSendQuery`.  We can then use it to send the points back to the
database, using the function `dbWriteTable`:

```r
query <- "CREATE SCHEMA test;"
dbSendQuery(con, query)
query <- "COMMENT ON SCHEMA test IS 'Schema for test purpose.';"
dbSendQuery(con, query)
dbWriteTable(con, c("test", "puechcirc"), dfpu)
```

We can now convert the coordinates into a proper geometry in PostGIS,
without forgetting to create a spatial index:

```r
query <- "ALTER TABLE test.puechcirc ADD COLUMN pts_geom geometry(POINT, 32632);"
dbSendQuery(con, query)
query <- "CREATE INDEX puechcirc_pts_geom_idx ON test.puechcirc USING GIST (pts_geom);"
dbSendQuery(con, query)
query <- "UPDATE test.puechcirc SET pts_geom=ST_SetSRID(ST_MakePoint(x, y), 32632)
WHERE x IS NOT NULL AND y IS NOT NULL;"
dbSendQuery(con, query)
query <- "COMMENT ON TABLE test.puechcirc IS 'Telemetry data (as points) from 2 wild boars at Puechabon (from RPostgreSQL).';"
dbSendQuery(con, query)
```


#### Exercise 8a: Export ltraj to the data base as steps

In this exercise, you will write the queries to use the ltraj
information to get the data as steps (i.e. segments of line) in the
database.


#### Exercise 8b: What about shapefiles?

But there's more than simple tables or points/steps in the
database. You will now try to import the roads (multilines) into R.


### 3.3.3 Topic 9: Introducing `rpostgis`

The package [`rpostgis`](https://mablab.org/rpostgis/) is an attempt
at facilitating the connection bewteen R and PostGIS. To this aim, it
introduces wrappers to common database procedures (e.g. `pgAddKey`,
`pgSchema`, `pgComment`, etc.), as well as functions dedicated to
handling spatial features.

The very first thing to do is to actually check that PostGIS has been
enabled on the database, and if not, enable it. This is exactly the
purpose of `pgPostGIS`:

```r
library("rpostgis")

pgPostGIS(con)
pgListGeom(con)
```

We now demonstrate `pgGetGeom`, to import any geometry (POINTS, LINES,
POLYGONS, etc.) into a `Spatial*DataFrame`. We use it to import roe
deer GPS data into `locs3`:

```r
locs3 <- pgGetGeom(con, c("main", "gps_data_animals"))
summary(locs3)
```

We could have selected only the subset of valid points directly, and
only specified columns:

```r
dbTableInfo(con, c("main", "gps_data_animals"))
locs3 <- pgGetGeom(con, c("main", "gps_data_animals"), other.cols = c("animals_id", 
    "acquisition_time", "longitude", "latitude", "roads_dist", 
    "altitude_srtm"), clauses = "WHERE gps_validity_code = 1")
summary(locs3)
```

We can now reproject the points to UTM 32, in order to compare them to
the locations imported at the very beginning of this lesson (in
`locs`):

```r
proj4string(locs3)
locs3 <- spTransform(locs3, CRS("+init=epsg:32632"))

all.equal(locs, locs3)
```


#### Exercise 9a: Import roads

Now it's time to get back on the road! Let's import the road as a
`SpatialLinesDataFrame` in R, and plot it with the GPS locations on
top of it. Try to be creative!


#### Exercise 9b: Export to PostGIS

Using functions from `rpostgis`, export `locs3` to PostGIS: first, use
`dbDrop` to drop the `test` schema, then recreate it with `dbSchema`,
comment it with `dbComment` and export the data as `locs_pg` using
`pgInsert`. In a second step, as an alternative to `pgInsert`, use 
a mix of `dbWriteTable`, `dbIndex` and `pgMakePts` (and `dbComment`!)
to export the same data as `locs_r`.


### 3.3.4 Topic 10: More of `rpostgis`

`rpostgis` provides a few additional features. For instance, you can
extract the bounding box of any geometry table:

```r
pgGetBoundary(con, c("main", "gps_data_animals"))
(bound <- pgGetBoundary(con, c("env_data", "study_area")))
```

As we saw from the previous lesson, `rpostgis` also provides
additional functions to read and also write rasters, which are stored
as `raster`s from the package `raster`:

```r
corine <- pgGetRast(con, c("env_data", "corine_land_cover"))
plot(corine)

dem <- pgGetRast(con, c("env_data", "srtm_dem"))
plot(dem)
plot(bound, add = TRUE)
dem <- pgGetRast(con, c("env_data", "srtm_dem"), boundary = bound)

sa <- pgGetGeom(con, c("env_data", "study_area"))
plot(dem)
plot(sa, add = TRUE)
points(locs3$longitude, locs3$latitude, pch = 3)
```

We can also write rasters to the database, using `pgWriteRast`. Here's
an example where we first reclassify the DEM in R into two classes,
and then send the reclassified raster to the database:

```r
(rec <- data.frame(from = c(0, 1000), to = c(1000, 5000), becomes = c(1,2)))

# reclassify to new raster
library("raster")
ele.rec <- reclassify(dem, rec)
plot(ele.rec)

# send to database
pgWriteRast(con, c("test", "elev_reclass"), raster = ele.rec)
```

Finally, while `dbReadTable` and `dbWriteTable` are really
"table"-oriented, they lose `data.frame`s attributes created in R,
which can be a source of errors when the database is used to store
data from R. To circumvent that problem, `rpostgis` proposes the
equivalent functions for `data.frame`s: `dbReadDataFrame` and
`dbWriteDataFrame`. To understand the difference, check the following
code:

```r
head(dfpu)

dbWriteTable(con, c("test", "dfpu_table"), dfpu)
df1 <- dbReadTable(con, c("test", "dfpu_table"))
head(df1)

dbWriteDataFrame(con, c("test", "dfpu"), dfpu)
df2 <- dbReadDataFrame(con, c("test", "dfpu"))
head(df2)

str(df1)
str(df2)

attributes(df1$date)
attributes(df2$date)
```


### 3.3.5 Topic 11: Let's not forget about trajectories, here comes `rpostgisLT`

`rpostgisLT` is a simple package that only does two things, but do it
well:

* Implement a `pgtraj` data structure in PostGIS that mirrors `ltraj`
  in R, allowing bidirectional transfer between the two;
* Visualize a `pgtraj` dynamically, through a web interface.

The easiest way to create a `pgtraj` is directly from a `ltraj`
object. For later visualization, the `ltraj` needs to have the
associated projection data in a `CRS` object (this is not something
that we can do later at this stage):

```r
library("adehabitatLT")
data(puechcirc)
attr(puechcirc, "proj4string") <- CRS("+init=epsg:27573")

library("rpostgisLT")
ltraj2pgtraj(con, puechcirc)
```

With this approach, the trajectory is simply stored in the database,
and can be retrieved without any modification:

```r
puech2 <- pgtraj2ltraj(con, "puechcirc")
all.equal(puechcirc, puech2)
```

The trajectory in the DB can now be dynamically explored with the
function `explorePgtraj`:

```r
explorePgtraj(conn = con, schema = "traj", pgtraj = "puechcirc")
```


#### Exercice 11: Create a trajectory in the DB with `asPgtraj`

Use the roe deer data to create a `pgtraj` directly in the DB with the
function `asPgtraj`. Check the help for this function to find out how
to use it. You can then explore it with `explorePgtraj`.


### 3.3.6 Closure…

Finally, we don't forget to close the connection to the database, as
opened by RPostgreSQL at the beginning of this exercise:

```r
dbDisconnect(con)
```


## 3.4 Extending PostGIS with Pl/R (DEMO)

Until now, we saw what PostGIS had to offer to process and manage
trackign data, and how R could be used to analyse movement data. We
also explored different ways to transfer data from PostGIS to R and
vice versa, which highlighted the many hurdles and difficulties in
streamlining the workflow. Pl/R is one possible answer to this
problem.

Pl/R is a loadable procedural language that allows the use of the R
engine and libraries directly inside the database, thus embedding R
scripts into SQL statements and database functions and triggers.

This lesson is a simplified version of Chapter 11 ("A Step Further in
the Integration of Data Management and Analysis: Pl/R") from the book
Spatial Database for GPS Wildlife Tracking Data (Urbano & Cagnacci,
2014).


### 3.4.1 Getting Started with Pl/R

From now on, we assume that Pl/R is properly installed on your system
(which may be a tricky process). We can then enable Pl/R in the
database, and check the outcome like this:

```sql
CREATE EXTENSION plr;
SELECT * FROM plr_version();
```

Now you can create functions in Pl/R procedural language pretty much
the same way you write functions in R. Indeed, the body of a Pl/R
function uses the R syntax, because it is actually pure R code! A
generic R code snippet such as:

```r
> x <- 10
> 4/3*pi*x^3
```

can be directly embedded into a Pl/R function in PostgreSQL using a
generic function skeleton with the Pl/R language:

```sql
CREATE OR REPLACE FUNCTION tools.plr_fn ()
RETURNS float8 AS
$BODY$
  x <- 10
  4/3*pi*x^3
$BODY$
LANGUAGE 'plr';
```

The function can then be used in an SQL statement:

```sql
SELECT tools.plr_fn ();
```

Fortunately, we can also have more useful pieces of code in Pl/R. In
this workflow, however, we still need to communicate data from the
database to and from R. Pl/R can natively handle several types,
including booleans (converted to `logical` in R), all forms of integer
(converted to `integer`) or numeric (converted to `numeric`) and all
forms of text (converted to `character`).

In a simple example, let's try to compute logarithms. We write a Pl/R
function `r_log` to calculate the logarithm of a sample of numbers
using R:

```sql
CREATE OR REPLACE FUNCTION tools.r_log(float8, float8)
RETURNS float AS
$BODY$
  log(arg1, arg2)
$BODY$
LANGUAGE 'plr';
```

Note that with a Pl/R function, the R engine does the computation, and
PostgreSQL only handles the input and output, so that we can compare
the outputs to the same logarithms computed by PostgreSQL:

```sql
SELECT
  ST_Area(geom) AS area,
  log(ST_Area(geom)) AS pg_log,
  tools.r_log(ST_Area(geom), 10) AS r_log,
  ln(ST_Area(geom)) AS pg_ln,
  tools.r_log(ST_Area(geom), exp(1)) AS r_ln
FROM analysis.view_convex_hulls;      
```

Fortunately, the results seem consistent…


### 3.4.2 In the Middle of the Night

One of the most powerful assets of R is its broad and ever-growing
package ecosystem. In this example, you are going to implement a
useful feature concealed in the `maptools` package, which provides a
set of functions able to deal with the position of the sun and compute
crepuscule, sunrise and sunset times for a given location at a given
date. We thus need to install in R the `maptools` package, together
with `rgeos` and `rgdal` packages (which should already be there after
the previous lesson):

```r
install.packages(c("rgeos", "rgdal", "maptools"))
```

Pl/R can communicate basic data types from PostgreSQL and R, but
cannot handle spatial objects. To circumvent this problem, we will use
well-known text (WKT) representations, which are simply passed as text
strings. Here is the daylight function, which returns the sunrise and
sunset times (as a text array) for a spatial point expressed as a WKT,
with its associated SRID, a timestamp to give the date and a time
zone:

```sql
CREATE OR REPLACE FUNCTION tools.daylight(
  wkt text,
  srid integer,
  datetime timestamptz,
  timezone text)
RETURNS text[] AS
$BODY$
  require(rgeos)
  require(maptools)
  require(rgdal)
  pt <- readWKT(wkt, p4s = CRS(paste0("+init=epsg:", srid)))
  dt <- as.POSIXct(substring(datetime, 1, 19), tz = timezone)
  sr <- sunriset(pt, dateTime = dt, direction = "sunrise",
      POSIXct.out = TRUE)$time
  ss <- sunriset(pt, dateTime = dt, direction = "sunset",
      POSIXct.out = TRUE)$time
  return(c(as.character(sr), as.character(ss)))
$BODY$
LANGUAGE 'plr';
```

Let's get the sunrise and sunset times for today, near the
municipality of Terlago, northern Italy. Because R and PostgreSQL use
different time zone formats, you need to pass the time zone to R
literally as `Europe/Rome`:

```sql
SELECT tools.daylight('POINT(11.001 46.001)', 4326, now(), 'Europe/Rome');
```

Let's now modify this function to return a boolean value (`TRUE` or
`FALSE`) indicating whether a given time of the day at a given
location corresponds to daylight or not. This is the purpose of the
`is_daylight` function, which will prove useful to test the daylight
for animal locations:

```sql
CREATE OR REPLACE FUNCTION tools.is_daylight(
  wkt text,
  srid integer,
  datetime timestamptz,
  timezone text)
RETURNS boolean AS
$BODY$
  require(rgeos)
  require(maptools)
  require(rgdal)
  pt <- readWKT(wkt, p4s = CRS(paste0("+init=epsg:", srid)))
  dt <- as.POSIXct(substring(datetime, 1, 19), tz = timezone)
  sr <- sunriset(pt, dateTime = dt, direction = "sunrise",
      POSIXct.out = TRUE)$time
  ss <- sunriset(pt, dateTime = dt, direction = "sunset",
      POSIXct.out = TRUE)$time
  return(ifelse(dt >= sr & dt < ss, TRUE, FALSE))
$BODY$
LANGUAGE 'plr';
```

This function can be used on a single point, e.g. with the same
coordinates as above:

```sql
SELECT tools.is_daylight('POINT(11.001 46.001)', 4326, current_date, 'Europe/Rome');
```

Or it can be used to a series of points, such as the first 10 valid
locations:

```sql
WITH tmp AS (SELECT ('Europe/Rome')::text AS tz)
SELECT
  ST_AsText(geom) AS location,
  acquisition_time AT TIME ZONE tz AS acquisition_time,
  tools.is_daylight(ST_AsText(geom), ST_SRID(geom), acquisition_time AT TIME ZONE tz, tz)
FROM main.gps_data_animals, tmp
WHERE gps_validity_code = 1
LIMIT 10;
```


### 3.4.2 Extending the Home Range Concept

In this section, we will embed the function `kernelUD` (from
`adehabitatHR`) to compute kernel home ranges directly from
PostGIS. We start by installing the package in R if necessary:

```r
install.packages("adehabitatHR")
```

We want to be able to produce the home range contours as produced by
the kernel utilization distribution (see the help page for the
function `kernelUD` in R). To do this, we first need to create a new
type `hr` that stores a polygon as a WKT, together with its associated
percentage (e.g. 90% corresponds to the area with a 90% probability of
finding the animal of interest):

```sql
CREATE TYPE tools.hr AS (percent int, wkt text);
```

We thus create the function `tools.kernelud` to compute kernel home
ranges using R:

```sql
CREATE OR REPLACE FUNCTION tools.kernelud (wkt text, percent integer)
RETURNS SETOF tools.hr AS
$BODY$
  require(rgeos)
  require(adehabitatHR)
  geom <- readWKT(wkt)
  kud <- kernelUD(geom)
  return(data.frame(percent = percent, wkt = sapply(percent, function(x)
      writeWKT(getverticeshr(kud, x)))))
$BODY$
LANGUAGE plr;
```

We can now query the table with all animal locations to compute the
kernel home range, for instance for animal 1 at 50, 90, and 95% (note
that we only use valid locations, as usual):

```sql
WITH tmp AS (SELECT unnest(ARRAY[50,90,95]) AS pc)
SELECT (tools.kernelud(ST_AsText(ST_Collect(ST_Transform(geom, 32632))), pc)).*
FROM main.gps_data_animals, tmp
WHERE animals_id = 1 AND gps_validity_code = 1
GROUP BY pc
ORDER BY pc;
```

Finally, we can now create a table `analysis.home_ranges_kernelud` to
store the different kernel home ranges and various parameters of
interest:

```sql
CREATE TABLE analysis.home_ranges_kernelud(
  home_ranges_kernelud_id serial NOT NULL,
  animals_id integer NOT NULL,
  start_time timestamp with time zone NOT NULL,
  end_time timestamp with time zone NOT NULL,
  num_locations integer,
  area numeric(13,5),
  geom geometry (multipolygon, 32632),
  percentage double precision,
  insert_timestamp timestamp with time zone DEFAULT now(),
  CONSTRAINT home_ranges_kernelud_pk
    PRIMARY KEY (home_ranges_kernelud_id),
  CONSTRAINT home_ranges_kernelud_animals_fk
    FOREIGN KEY (animals_id)
    REFERENCES main.animals (animals_id) MATCH SIMPLE
    ON UPDATE NO ACTION ON DELETE NO ACTION);
COMMENT ON TABLE analysis.home_ranges_kernelud
IS 'Table that stores the home range polygons derived from kernelUD. The area is computed in squared km.';
CREATE INDEX fki_home_ranges_kernelud_animals_fk
  ON analysis.home_ranges_kernelud
  USING btree (animals_id);
CREATE INDEX gist_home_ranges_kernelud_index
  ON analysis.home_ranges_kernelud
  USING gist (geom);
```

Let us now populate this table using 50 and 90% kernels for all
animals except number 6:

```sql
WITH
  tmp AS (SELECT unnest(ARRAY[50,90,95]) AS pc),
  kud AS (
    SELECT
      animals_id,
      min(acquisition_time) AS start_time,
      max(acquisition_time) AS end_time,
      count(animals_id) AS num_locations,
      (tools.kernelud(ST_AsText(ST_Collect(ST_Transform(geom, 32632))), pc)).*
    FROM main.gps_data_animals, tmp
    WHERE gps_validity_code = 1
      AND animals_id <> 6
    GROUP BY animals_id,pc
    ORDER BY animals_id,pc)
INSERT INTO analysis.home_ranges_kernelud (animals_id, start_time, end_time,
  num_locations, area, geom, percentage)
SELECT
  animals_id,
  start_time,
  end_time,
  num_locations,
  ST_Area(wkt) / 1000000,
  ST_GeomFromText(wkt, 32632),
  percent / 100.0
FROM kud
ORDER BY animals_id, percent;
```

We can display part of the outcome in this table, for instance to
check that the area is increasing with higher percentage:

```sql
SELECT animals_id, percentage, num_locations, area
FROM analysis.home_ranges_kernelud
ORDER BY animals_id, percentage;
```

… and explore in QGIS the different polygons associated to different
percentages for each animal, with the GPS locations overlaid.


### 3.4.4 Concluding remarks about Pl/R

The benefits of Pl/R are obvious: it allows to embed R (and its myriad
of packages) into the database, thus enhancing the database itself
with R superpowers! One of the biggest advantage of this approach is
to streamline procedures that would involve going back and forth from
PostGIS to R: imagine for instance a scenario where you would prepare
a trajectory in PostGIS, import it in R to compute Brownian bridge
kernels, which you would export back to PostGIS to intersect them with
environmental GIS layers (e.g. Corine Land Cover), before sending the
results of the intersection to R again for further analysis… Using
Pl/R, you could easily embed the R functions required to compute
Brownian bridge kernels, and thus having a completely smooth workflow.

The limits of the approach are, unfortunately, exactly the same as for
connecting R to PostGIS: communicating complex spatial objects can be
a real issue (e.g. using `rgdal` or manipulating rasters in
general). However, we can only expect progress in this area.



## 3.5 Recap exercises


### 3.5.1 Pure R

Generate a correlated random walk of 1000 steps, with a concentration
parameter of `r = 0.6`, and a seed for the random number generator of
`1234`. From there, rediscretize the trajectory every 10 seconds. What
is the path length ratio between the rediscretized random walk and the
original one?


### 3.5.2 R and PostGIS interaction

Using only R (with `RPostgreSQL` and `rpostgis`) to interact with your
PostGIS database, import the roe deer data, keeping only valid
locations.

From there:

1. Compute 50%, 90%, and 100% minimum convex polygons (MCP) for each
   individual.
2. Export all resulting polygons in a dedicated table in PostGIS,
   keeping the information about the individual, the method, and the
   percentage associated to the home range estimate.
3. Prepare a SQL query (in R!) to compare, for each individual, the
   area of the 100% MCP computed in R, and the same estimation using
   PostGIS (which calls it a convex hull), and retrieve the result in
   R. Are the two software consistent?
4. Prepare a SQL query (in R!) to intersect each MCP (50%, 90% and
   100%) with the town of Terlago (46.001°N; 11.001°E), and retrieve
   the identity of individuals that include Terlago in their home
   range. Map it in R to show the home range(s) overlapping with
   Terlago. 
